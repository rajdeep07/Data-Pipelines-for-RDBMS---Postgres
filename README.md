# Project 1: Data Modelling with Postgres - Song Data Analysis
[![Project Passed](https://img.shields.io/badge/project-passed-success.svg)](https://img.shields.io/badge/project-passed-success.svg)

## Summary
* [Schema definition](#Schema-definition)
* [How to run](#How-to-run)
* [Project structure](#Project-structure)
* [Example queries](#Example-queries)
--------------------------------------------


### Schema definition
This is the schema of the database

Fact Table: 
* songplays - records in log data associated with song plays i.e. records with page NextSong <br>
* songplay_id, start_time, user_id, level, song_id, artist_id, session_id, location, user_agent <br>


Dimension Table:
* users - users in the app <br>
* user_id, first_name, last_name, gender, level <br>
* songs - songs in music database <br>
* song_id, title, artist_id, year, duration <br>
* artists - artists in music database <br>
* artist_id, name, location, latitude, longitude <br>
* time - timestamps of records in songplays broken down into specific units <br>
* start_time, hour, day, week, month, year, weekday <br>

--------------------------------------------

#### How to run
First of all, you need a PostgreSQL instance up and running <br>
Following tools are required to run the ETL process = Postgresql + Python <br>

<b> Note: </b><br>
In this example we will use user-password authorization mechanism

After installing your database on your local machine, you have to create a custom user called `student` with password `student` <br>
and create a database called `sparkifydb`

After opening terminal session, set your filesystem on project root folder <br>
and  insert these commands in order to run the demo: <br><br>
<I> Create Tables </I> <br>
Write CREATE statements in sql_queries.py to create each table. <br>
Write DROP statements in sql_queries.py to drop each table if it exists. <br>
Run create_tables.py to create your database and tables. <br>
Run test.ipynb to confirm the creation of your tables with the correct columns. Make sure to click "Restart kernel" to close the connection to the database after running this notebook. 

<I> Build ETL Processes </I> <br>
Follow instructions in the etl.ipynb notebook to develop ETL processes for each table. At the end of each table section, <br>
or at the end of the notebook, run test.ipynb to confirm that records were successfully inserted into each table. Remember <br>
to rerun create_tables.py to reset your tables before each time you run this notebook. 

<I> Build ETL Pipeline </I> <br>
Use what you've completed in etl.ipynb to complete etl.py, where you'll process the entire datasets. Remember to run create_tables.py <br>
before running etl.py to reset your tables. Run test.ipynb to confirm your records were successfully inserted into each table.

----------------------------

#### Project structure
This is the project structure, if the bullet contains ``/`` <br>
means that the resource is a folder:

* <b> /data </b> - Source of the JSON file, all these files have to be elaborated
  * <b> /log_data </b> - A folder that contains files of log files in JSON format generated by this [event simulator](https://github.com/Interana/eventsim) based on the songs in the dataset above. These simulate app activity logs from a music streaming app based on specified configurations.
  * <b> /song_data </b> -  Each file is in JSON format and contains metadata about a song and the artist of that song. The files are partitioned by the first three letters of each song's track ID
* <b> /imgs </b> - Simply a folder with images that are used in this ``md``
* <b> etl.ipynb </b> - It is a notebook that helps to know step by step what etl.py does
* <b> test.ipynb </b> - It is a notebook that helps to know if tables
  <br> are created and data are ingested correctly 
* <b> create_tables.py </b> - This script will drop old tables (if exist) ad re-create new tables
* <b> etl.py </b> - This script will read JSON every file contained in /data folder, parse them, <br> build relations though logical process and ingest data 
* <b> sql_queries.py </b> - This file contains variables with SQL statement in String formats, <br> partitioned by CREATE, DROP, INSERT statements plus a FIND query 

----------------------------

#### Example queries

<I> For strategical purposes you may want to know which is the most frequenct location for songs</I>
``` SQL
SELECT location, count(location) FROM songplays GROUP BY location;
```

<I> For business purposes identifying who has sung most number of songs </I>
``` SQL
SELECT artists.artist_id, count(song_id) from songplays inner join artists on songplays.artist_id = artists.artist_id group by artists.artist_id;
```

----------------------------